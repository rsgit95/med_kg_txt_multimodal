{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCzMtobduJtf"
   },
   "source": [
    "# **1. Run Depth First Search on KG** \n",
    "\n",
    "Root is an admission node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "ROOT_DIR = 'dxprx'\n",
    "NUM_SPECIAL_TOKENS = 3\n",
    "eval_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. Not unified abstract embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119438,
     "status": "ok",
     "timestamp": 1604240640911,
     "user": {
      "displayName": "Park Sungjin",
      "photoUrl": "",
      "userId": "10892187777297360592"
     },
     "user_tz": -540
    },
    "id": "M6P8d-FytHkS",
    "outputId": "535c80ac-9331-4297-9e8c-49ce8dbb5bf1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 126924/1171955 [00:00<00:00, 1269229.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocessing\n",
      "level:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171955/1171955 [00:00<00:00, 1221176.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589909/1171955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 78429/589909 [00:00<00:00, 784277.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 589909/589909 [00:00<00:00, 800492.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7863/589909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7863/7863 [00:00<00:00, 745559.23it/s]\n",
      "  8%|▊         | 2766/32696 [00:00<00:01, 27658.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level:2\n",
      "0/7863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32696/32696 [00:01<00:00, 27073.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32915\n",
      "240\n",
      "num_literals : 7863\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import spacy, scispacy\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def get_childs(subgraph, depth, heads):\n",
    "    temp_seq = list()\n",
    "    for head in heads:\n",
    "        temp_seq += subgraph[depth][head]\n",
    "    return temp_seq\n",
    "\n",
    "# Build dictionaries from file\n",
    "triples = [x.split() for x in open(os.path.join(ROOT_DIR,'train2id.txt')).read().splitlines()[1:]]\n",
    "node2edge = {(h,t):r for h,t,r in triples}\n",
    "nodes = {x.split('\\t')[0]:x.split('\\t')[-1] for x in open(os.path.join(ROOT_DIR,'entity2id.txt')).read().splitlines()[1:]}\n",
    "literals = {k:int(v)+NUM_SPECIAL_TOKENS for (k,v) in list(nodes.items()) if '^^' in k}\n",
    "edges = {x.split()[0]:x.split()[1] for x in open(os.path.join(ROOT_DIR,'relation2id.txt')).read().splitlines()[1:]}\n",
    "\n",
    "# Extract Admission Nodes & Literals\n",
    "adm_node = list()\n",
    "for node in list(nodes.items()):\n",
    "    if 'hadm' in node[0]:\n",
    "        adm_node.append(node[1])\n",
    "        \n",
    "# Initialize subgraph\n",
    "subgraph_norel = [{node:list() for node in adm_node}]\n",
    "\n",
    "# Depth First Search\n",
    "print('start preprocessing')\n",
    "level = 0\n",
    "while len(triples)>0:\n",
    "    queue = list()\n",
    "    print('level:{}'.format(level))\n",
    "    for triple in tqdm(triples):\n",
    "        if triple[0] in subgraph_norel[level]:\n",
    "            subgraph_norel[level][triple[0]].append(triple[1])\n",
    "            flag = False\n",
    "        else:\n",
    "            flag = True\n",
    "        if flag:\n",
    "            queue.append(triple)\n",
    "    print('{}/{}'.format(len(queue),len(triples)))\n",
    "    new_head = list()\n",
    "    for heads in list(subgraph_norel[level].values()):\n",
    "        new_head+=heads\n",
    "    subgraph_norel.append({k:list() for k in new_head})\n",
    "    triples = queue\n",
    "    level += 1\n",
    "    if level > 30:\n",
    "        break\n",
    "\n",
    "# Build subgraph\n",
    "subgraphs = dict()\n",
    "max_len = 239\n",
    "cnt = 0\n",
    "for head in tqdm(list(subgraph_norel[0].keys())):\n",
    "    depth=0\n",
    "    seq = [head]\n",
    "    heads = [head]\n",
    "    while depth<level:\n",
    "        heads = get_childs(subgraph_norel,depth,heads)\n",
    "        seq += heads\n",
    "        depth+=1\n",
    "    if len(seq)>max_len:\n",
    "        continue\n",
    "    else:\n",
    "        subgraphs[head]=[2]+[int(x)+NUM_SPECIAL_TOKENS for x in seq]+[0]*(max_len-len(seq))\n",
    "\n",
    "\n",
    "# Align subgraph and note, remove unmathced samples\n",
    "#aid = [nodes['</hadm_id/{}>'.format(x)] for x in open(os.path.join(ROOT_DIR,'p_hadm_ids.txt')).read().splitlines() if (len(x)>0) and ('</hadm_id/{}>'.format(x) in nodes)]\n",
    "#note = [x for x in torch.load(os.path.join(ROOT_DIR,'p_sections.txt'))if (len(x)>0)]\n",
    "# Load and preprocess note\n",
    "note_aid_pair = list()\n",
    "f = torch.load(os.path.join(ROOT_DIR,'p_sections'))\n",
    "#print(len(f))\n",
    "#print(f[0])\n",
    "for aid, note in torch.load(os.path.join(ROOT_DIR,'p_sections')):\n",
    "    try:\n",
    "        if nodes[f'</hadm_id/{aid}>'] in subgraphs:\n",
    "            note_refined = {header.replace('\"',''):' '.join([token.text for token in nlp(text.replace('\"',''))]) for header, text in note.items()}\n",
    "            note_aid_pair.append((nodes[f'</hadm_id/{aid}>'],note_refined))\n",
    "    except:\n",
    "        continue\n",
    "#print('{}/{}'.format(len(aid),len(adm_node)))\n",
    "#print(len(note))\n",
    "print(len(note_aid_pair))\n",
    "print(max(list(map(lambda x: len(x),list(subgraphs.values())))))\n",
    "print('num_literals : {}'.format(len(literals.items())))\n",
    "\n",
    "# Re-indexing nodes in current subgraph after filtering\n",
    "new_nodes = list()\n",
    "for head, note in note_aid_pair:\n",
    "    new_nodes += subgraphs[head]\n",
    "new_nodes = set(new_nodes)\n",
    "old2new = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_literals : 7863\n",
      "num_nodes : 630450\n"
     ]
    }
   ],
   "source": [
    "print('num_literals : {}'.format(len(literals.items())))\n",
    "print('num_nodes : {}'.format(len(nodes.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Build DB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-0. TVT split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Input\n",
    "if not os.path.exists('{}.db'.format(ROOT_DIR)):\n",
    "    DB = {'train':[],'valid':[],'test':[]}\n",
    "    for sample in tqdm(note_aid_pair):\n",
    "        split = np.random.choice(list(DB.keys()),p=[0.8,0.1,0.1])\n",
    "        if (len(DB[split])>=eval_size) and (split in ['valid', 'test']):\n",
    "            split = 'train'\n",
    "        DB[split].append(sample)\n",
    "    torch.save(DB,'{}.db'.format(ROOT_DIR))\n",
    "else:\n",
    "    DB = torch.load('{}.db'.format(ROOT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIoqHq_nueia"
   },
   "source": [
    "## 2-1-(1). Masked Literal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280336,
     "status": "ok",
     "timestamp": 1604241423375,
     "user": {
      "displayName": "Park Sungjin",
      "photoUrl": "",
      "userId": "10892187777297360592"
     },
     "user_tz": -540
    },
    "id": "T8yG81c8tHkW",
    "outputId": "2b36e7fd-a267-4346-d04c-fb5d0fe2b7b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:00, 12.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] set size : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 13.19it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 17.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] set size : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 15.99it/s]\n"
     ]
    }
   ],
   "source": [
    "task = '{}_NoKGenc'.format(ROOT_DIR)\n",
    "triples = [x.split() for x in open(os.path.join(ROOT_DIR,'train2id.txt')).read().splitlines()[1:]]\n",
    "node2edge = {(int(h)+NUM_SPECIAL_TOKENS,int(t)+NUM_SPECIAL_TOKENS):int(r) for h,t,r in triples}\n",
    "\n",
    "if not os.path.isdir(task):\n",
    "    os.mkdir(task)\n",
    "\n",
    "# Re-index literals for labeling\n",
    "literal_id2label = {k:v for (v,k) in enumerate(list(literals.values()))}\n",
    "torch.save(literal_id2label,'{}/id2label'.format(task))\n",
    "\n",
    "for split in DB:\n",
    "    ## Debugging Purpose\n",
    "    #if split == 'train':\n",
    "    #    continue\n",
    "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
    "    if not os.path.isdir(os.path.join(task,split)):\n",
    "        os.mkdir(os.path.join(task,split))\n",
    "    inputs = list()\n",
    "    labels = list()\n",
    "    label_masks = list()\n",
    "    rc_indeces = list()\n",
    "    notes = list()\n",
    "    for head, note in tqdm(DB[split]):\n",
    "        subgraph = subgraphs[head]\n",
    "        inputs.append(subgraph)\n",
    "        labels.append(list(map(lambda x: literal_id2label[x] if x in literal_id2label else -100,subgraph)))\n",
    "        label_masks.append(list(map(lambda x: 1 if x in literal_id2label else 0,subgraph)))\n",
    "        # Add RC index for sample\n",
    "        num_nodes = sum([1 for x in subgraph if x!=0])\n",
    "        rc_index = list()\n",
    "        not_conn = 0\n",
    "        while len(rc_index) < 0.1*num_nodes:\n",
    "            idx_pair = (random.randint(0,num_nodes-1),random.randint(0,num_nodes-1))\n",
    "            node_pair = (subgraph[idx_pair[0]], subgraph[idx_pair[1]])\n",
    "            inv_node_pair = (subgraph[idx_pair[1]], subgraph[idx_pair[0]])\n",
    "            if (idx_pair[0] == idx_pair[1]):\n",
    "                continue\n",
    "            if node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[node_pair],))\n",
    "            elif inv_node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[inv_node_pair],))\n",
    "            else:\n",
    "                if not_conn < 0.05*num_nodes:\n",
    "                    rc_index.append(idx_pair+(len(set(node2edge.values())),))\n",
    "                    not_conn +=1\n",
    "        rc_indeces.append(rc_index)\n",
    "        notes.append(note)\n",
    "    db = {'input':inputs,\n",
    "                'label':labels,\n",
    "                'label_mask':label_masks,\n",
    "                'rc_index':rc_indeces,\n",
    "                'text':notes}\n",
    "    torch.save(db,'{}/db'.format(os.path.join(task,split)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "['</hadm_id/129414>', '</diagnoses/406609>', '</diagnoses/406605>', '</diagnoses/406610>', '</diagnoses/406613>', '</diagnoses/406606>', '</diagnoses/406612>', '</diagnoses/406616>', '</diagnoses/406608>', '</diagnoses/406615>', '</diagnoses/406614>', '</diagnoses/406611>', '</diagnoses/406607>', '</diagnoses_icd9_code/2662>', '</diagnoses_icd9_code/30000>', '</diagnoses_icd9_code/486>', '</diagnoses_icd9_code/79029>', '</diagnoses_icd9_code/4019>', '</diagnoses_icd9_code/27652>', '</diagnoses_icd9_code/2859>', '</diagnoses_icd9_code/49390>', '</diagnoses_icd9_code/2761>', '</diagnoses_icd9_code/32723>', '</diagnoses_icd9_code/27800>', '</diagnoses_icd9_code/30500>', '\"other b-complex deficiencies\"', '\"anxiety state, unspecified\"', '\"pneumonia, organism unspecified\"', '\"other abnormal glucose\"', '\"unspecified essential hypertension\"', '\"hypovolemia\"', '\"anemia, unspecified\"', '\"asthma, unspecified type, unspecified\"', '\"hyposmolality and/or hyponatremia\"', '\"obstructive sleep apnea (adult)(pediatric)\"', '\"obesity, unspecified\"', '\"alcohol abuse, unspecified\"']\n",
      "label:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3520, 6164, 267, 2663, 4636, 248, 934, 6580, 7104, 4440, 3247, 3531, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "label_mask:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "rc_index:\n",
      "[('</diagnoses/406608>', '</diagnoses_icd9_code/27800>', 6), ('</diagnoses/406607>', '\"obstructive sleep apnea (adult)(pediatric)\"', 6), ('</diagnoses_icd9_code/32723>', '\"obstructive sleep apnea (adult)(pediatric)\"', 4), ('</hadm_id/129414>', '</diagnoses/406609>', 1)]\n",
      "text:\n",
      "{'dx': '  Hypoxia Community acquired pneumonia Obstructive sleep apnea Asthma Hypertension', 'prx': 'None'}\n"
     ]
    }
   ],
   "source": [
    "IDX = 1\n",
    "id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(ROOT_DIR,'entity2id.txt')).read().splitlines()[1:]}\n",
    "for k, v in db.items():\n",
    "    print(f'{k}:')\n",
    "    if k=='input':\n",
    "        print([id2entity[x] for x in v[IDX][1:] if x!=0])\n",
    "    elif k=='rc_index':\n",
    "        print([(id2entity[db['input'][IDX][h]],id2entity[db['input'][IDX][t]],r) for h,t,r in v[IDX]])\n",
    "    else:\n",
    "        print(v[IDX])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1-(3). Masked Literal Prediction, Graph Enc, UniKGenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:00,  9.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] set size : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.82it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 15.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] set size : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 14.58it/s]\n"
     ]
    }
   ],
   "source": [
    "task = '{}_UniKGenc'.format(ROOT_DIR)\n",
    "triples = [x.split() for x in open(os.path.join(ROOT_DIR,'train2id.txt')).read().splitlines()[1:]]\n",
    "node2edge = {(int(h)+NUM_SPECIAL_TOKENS,int(t)+NUM_SPECIAL_TOKENS):int(r) for h,t,r in triples}\n",
    "\n",
    "if not os.path.isdir(task):\n",
    "    os.mkdir(task)\n",
    "\n",
    "# Re-index literals for labeling\n",
    "literal_id2label = {k:v for (v,k) in enumerate(list(literals.values()))}\n",
    "torch.save(literal_id2label,'{}/id2label'.format(task))\n",
    "\n",
    "for split in DB:\n",
    "    ## Debugging Purpose\n",
    "    #if split == 'train':\n",
    "    #    continue\n",
    "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
    "    if not os.path.isdir(os.path.join(task,split)):\n",
    "        os.mkdir(os.path.join(task,split))\n",
    "    inputs = list()\n",
    "    masks = list()\n",
    "    labels = list()\n",
    "    label_masks = list()\n",
    "    rc_indeces = list()\n",
    "    notes = list()\n",
    "    \n",
    "    for head, note in tqdm(DB[split],total=len(DB[split])):\n",
    "        subgraph = subgraphs[head]\n",
    "        # Append input\n",
    "        inputs.append(subgraph)\n",
    "        # Append label\n",
    "        labels.append(list(map(lambda x: literal_id2label[x] if x in literal_id2label else -100,subgraph)))\n",
    "        label_masks.append(list(map(lambda x: 1 if x in literal_id2label else 0,subgraph)))\n",
    "        # Append attention mask for graph encoder\n",
    "        mask =  torch.eye(len(subgraph))\n",
    "        for head_idx, head in enumerate(subgraph):\n",
    "            for tail_idx, tail in enumerate(subgraph):\n",
    "                if head_idx>tail_idx:\n",
    "                    continue\n",
    "                elif (head==0) or (tail==0):\n",
    "                    continue\n",
    "                else:\n",
    "                    if (head,tail) in node2edge:\n",
    "                        mask[(head_idx, tail_idx)]=1.0\n",
    "                        mask[(tail_idx, head_idx)]=1.0\n",
    "        masks.append(mask)\n",
    "        # Add RC index for sample\n",
    "        num_nodes = sum([1 for x in subgraph if x!=0])\n",
    "        rc_index = list()\n",
    "        not_conn = 0\n",
    "        while len(rc_index) < 0.1*num_nodes:\n",
    "            idx_pair = (random.randint(0,num_nodes-1),random.randint(0,num_nodes-1))\n",
    "            node_pair = (subgraph[idx_pair[0]], subgraph[idx_pair[1]])\n",
    "            inv_node_pair = (subgraph[idx_pair[1]], subgraph[idx_pair[0]])\n",
    "            if (idx_pair[0] == idx_pair[1]):\n",
    "                continue\n",
    "            if node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[node_pair],))\n",
    "            elif inv_node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[inv_node_pair],))\n",
    "            else:\n",
    "                if not_conn < 0.05*num_nodes:\n",
    "                    rc_index.append(idx_pair+(len(set(node2edge.values())),))\n",
    "                    not_conn +=1\n",
    "        rc_indeces.append(rc_index)\n",
    "        # Append note for text encoder\n",
    "        notes.append(note)\n",
    "            \n",
    "    db = {'input':inputs,\n",
    "                'mask':masks,\n",
    "                'label':labels,\n",
    "                'label_mask':label_masks,\n",
    "                'text':notes,\n",
    "                'rc_index':rc_indeces}\n",
    "    torch.save(db,'{}/db'.format(os.path.join(task,split)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "['</hadm_id/129414>', '</diagnoses/406609>', '</diagnoses/406605>', '</diagnoses/406610>', '</diagnoses/406613>', '</diagnoses/406606>', '</diagnoses/406612>', '</diagnoses/406616>', '</diagnoses/406608>', '</diagnoses/406615>', '</diagnoses/406614>', '</diagnoses/406611>', '</diagnoses/406607>', '</diagnoses_icd9_code/2662>', '</diagnoses_icd9_code/30000>', '</diagnoses_icd9_code/486>', '</diagnoses_icd9_code/79029>', '</diagnoses_icd9_code/4019>', '</diagnoses_icd9_code/27652>', '</diagnoses_icd9_code/2859>', '</diagnoses_icd9_code/49390>', '</diagnoses_icd9_code/2761>', '</diagnoses_icd9_code/32723>', '</diagnoses_icd9_code/27800>', '</diagnoses_icd9_code/30500>', '\"other b-complex deficiencies\"', '\"anxiety state, unspecified\"', '\"pneumonia, organism unspecified\"', '\"other abnormal glucose\"', '\"unspecified essential hypertension\"', '\"hypovolemia\"', '\"anemia, unspecified\"', '\"asthma, unspecified type, unspecified\"', '\"hyposmolality and/or hyponatremia\"', '\"obstructive sleep apnea (adult)(pediatric)\"', '\"obesity, unspecified\"', '\"alcohol abuse, unspecified\"']\n",
      "mask:\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.])\n",
      "label:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3520, 6164, 267, 2663, 4636, 248, 934, 6580, 7104, 4440, 3247, 3531, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "label_mask:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "text:\n",
      "{'dx': '  Hypoxia Community acquired pneumonia Obstructive sleep apnea Asthma Hypertension', 'prx': 'None'}\n",
      "rc_index:\n",
      "[('</diagnoses_icd9_code/27800>', '\"asthma, unspecified type, unspecified\"', 6), ('</diagnoses_icd9_code/32723>', '</diagnoses/406615>', 6), ('</diagnoses_icd9_code/2761>', '</diagnoses/406615>', 0), ('</diagnoses_icd9_code/4019>', '</diagnoses/406606>', 0)]\n"
     ]
    }
   ],
   "source": [
    "IDX = 1\n",
    "id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(ROOT_DIR,'entity2id.txt')).read().splitlines()[1:]}\n",
    "for k, v in db.items():\n",
    "    print(f'{k}:')\n",
    "    if k=='input':\n",
    "        print([id2entity[x] for x in v[IDX][1:] if x!=0])\n",
    "    elif k=='mask':\n",
    "        print(v[IDX][1])\n",
    "    elif k=='rc_index':\n",
    "        print([(id2entity[db['input'][IDX][h]],id2entity[db['input'][IDX][t]],r) for h,t,r in v[IDX]])\n",
    "    else:\n",
    "        print(v[IDX])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2-(1). Unified Abstract Embedding, NoKGenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unified nodes : 7871\n",
      "[valid] set size : 10\n",
      "[test] set size : 10\n"
     ]
    }
   ],
   "source": [
    "task = '{}_UnifiedNoKGenc'.format(ROOT_DIR)\n",
    "triples = [x.split() for x in open(os.path.join(ROOT_DIR,'train2id.txt')).read().splitlines()[1:]]\n",
    "node2edge = {(int(h)+NUM_SPECIAL_TOKENS,int(t)+NUM_SPECIAL_TOKENS):int(r) for h,t,r in triples}\n",
    "\n",
    "if not os.path.isdir(task):\n",
    "    os.mkdir(task)\n",
    "\n",
    "# Re-index literals for labeling\n",
    "#literal_id2label = {k:v for (v,k) in enumerate(list(literals.values()))}\n",
    "#torch.save(literal_id2label,'{}/id2label'.format(task))\n",
    "\n",
    "# Abstract Node Unification\n",
    "node2uninode = {k:k for k in range(NUM_SPECIAL_TOKENS)}\n",
    "if ROOT_DIR == 'px':\n",
    "    unified_node= {'PAD':0,'MASK':1,'CLS':2,'hadm':3,'prescript':4,'icustay':5}\n",
    "    for key in nodes:\n",
    "        if key in literals:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = len(unified_node)\n",
    "            unified_node[key]=len(unified_node)\n",
    "        elif 'hadm' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['hadm']\n",
    "        elif 'prescript' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['prescript']\n",
    "        elif 'icustay' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['icustay']\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "elif ROOT_DIR == 'dxprx':\n",
    "    unified_node= {'PAD':0,'MASK':1,'CLS':2,'hadm':3,'diagnoses_icd9_code':4,'diagnoses':5,'procedures_icd9_code':6,'procedures':7}\n",
    "    for key in nodes:\n",
    "        if key in literals:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = len(unified_node)\n",
    "            unified_node[key]=len(unified_node)     \n",
    "        elif 'hadm' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['hadm']\n",
    "        elif 'diagnoses_icd9_code' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['diagnoses_icd9_code']\n",
    "        elif 'diagnoses' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['diagnoses']\n",
    "        elif 'procedures_icd9_code' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['procedures_icd9_code']\n",
    "        elif 'procedures' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['procedures']\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "torch.save(unified_node,'{}/unified_node'.format(task))\n",
    "torch.save(node2uninode,'{}/node2uninode'.format(task))\n",
    "print('# Unified nodes : {}'.format(len(unified_node)))\n",
    "            \n",
    "for split in DB:\n",
    "    ## Debugging Purpose\n",
    "    #if split == 'train':\n",
    "    #    continue\n",
    "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
    "    if not os.path.isdir(os.path.join(task,split)):\n",
    "        os.mkdir(os.path.join(task,split))\n",
    "    inputs = list()\n",
    "    labels = list()\n",
    "    label_masks = list()\n",
    "    rc_indeces = list()\n",
    "    notes = list()\n",
    "    for head, note in tqdm(DB[split]):\n",
    "        subgraph = subgraphs[head]\n",
    "        inputs.append(list(map(lambda x: node2uninode[x],subgraph)))\n",
    "        labels.append(list(map(lambda x: node2uninode[x] if x in literal_id2label else -100,subgraph)))\n",
    "        label_masks.append(list(map(lambda x: 1 if x in literal_id2label else 0,subgraph)))\n",
    "        # Add RC index for sample\n",
    "        num_nodes = sum([1 for x in subgraph if x!=0])\n",
    "        rc_index = list()\n",
    "        not_conn = 0\n",
    "        while len(rc_index) < 0.1*num_nodes:\n",
    "            idx_pair = (random.randint(0,num_nodes-1),random.randint(0,num_nodes-1))\n",
    "            node_pair = (subgraph[idx_pair[0]], subgraph[idx_pair[1]])\n",
    "            inv_node_pair = (subgraph[idx_pair[1]], subgraph[idx_pair[0]])\n",
    "            if (idx_pair[0] == idx_pair[1]):\n",
    "                continue\n",
    "            if node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[node_pair],))\n",
    "            elif inv_node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[inv_node_pair],))\n",
    "            else:\n",
    "                if not_conn < 0.05*num_nodes:\n",
    "                    rc_index.append(idx_pair+(len(set(node2edge.values())),))\n",
    "                    not_conn +=1\n",
    "        rc_indeces.append(rc_index)\n",
    "        notes.append(note)\n",
    "    db = {'input':inputs,\n",
    "                'label':labels,\n",
    "                'label_mask':label_masks,\n",
    "                'rc_index':rc_indeces,\n",
    "                'text':notes}\n",
    "    torch.save(db,'{}/db'.format(os.path.join(task,split)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "['CLS', 'hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', '\"other b-complex deficiencies\"', '\"anxiety state, unspecified\"', '\"pneumonia, organism unspecified\"', '\"other abnormal glucose\"', '\"unspecified essential hypertension\"', '\"hypovolemia\"', '\"anemia, unspecified\"', '\"asthma, unspecified type, unspecified\"', '\"hyposmolality and/or hyponatremia\"', '\"obstructive sleep apnea (adult)(pediatric)\"', '\"obesity, unspecified\"', '\"alcohol abuse, unspecified\"']\n",
      "label:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3528, 6172, 275, 2671, 4644, 256, 942, 6588, 7112, 4448, 3255, 3539, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "label_mask:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "rc_index:\n",
      "[('\"obstructive sleep apnea (adult)(pediatric)\"', 'diagnoses', 6), ('\"obstructive sleep apnea (adult)(pediatric)\"', 'diagnoses_icd9_code', 6), ('hadm', 'diagnoses', 1), ('\"other b-complex deficiencies\"', 'diagnoses_icd9_code', 4)]\n",
      "text:\n",
      "{'dx': '  Hypoxia Community acquired pneumonia Obstructive sleep apnea Asthma Hypertension', 'prx': 'None'}\n"
     ]
    }
   ],
   "source": [
    "IDX = 1\n",
    "id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(ROOT_DIR,'entity2id.txt')).read().splitlines()[1:]}\n",
    "uninode2name = {v:k.split('^^')[0] for k,v in unified_node.items()}\n",
    "for k, v in db.items():\n",
    "    print(f'{k}:')\n",
    "    if k=='input':\n",
    "        print([uninode2name[x] for x in v[IDX] if x!=0])\n",
    "    elif k=='rc_index':\n",
    "        print([(uninode2name[db['input'][IDX][h]],uninode2name[db['input'][IDX][t]],r) for h,t,r in v[IDX]])\n",
    "    else:\n",
    "        print(v[IDX])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2-(2). Unified Abstract Embedding, UniKGenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:00,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unified nodes : 7871\n",
      "[valid] set size : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 12.09it/s]\n",
      " 20%|██        | 2/10 [00:00<00:00, 15.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] set size : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 14.37it/s]\n"
     ]
    }
   ],
   "source": [
    "task = '{}_UnifiedUniKGenc'.format(ROOT_DIR)\n",
    "triples = [x.split() for x in open(os.path.join(ROOT_DIR,'train2id.txt')).read().splitlines()[1:]]\n",
    "node2edge = {(int(h)+NUM_SPECIAL_TOKENS,int(t)+NUM_SPECIAL_TOKENS):int(r) for h,t,r in triples}\n",
    "\n",
    "if not os.path.isdir(task):\n",
    "    os.mkdir(task)\n",
    "\n",
    "# Re-index literals for labeling\n",
    "literal_id2label = {k:v for (v,k) in enumerate(list(literals.values()))}\n",
    "torch.save(literal_id2label,'{}/id2label'.format(task))\n",
    "\n",
    "# Abstract Node Unification\n",
    "node2uninode = {k:k for k in range(NUM_SPECIAL_TOKENS)}\n",
    "if ROOT_DIR == 'px':\n",
    "    unified_node= {'PAD':0,'MASK':1,'CLS':2,'hadm':3,'prescript':4,'icustay':5}\n",
    "    for key in nodes:\n",
    "        if key in literals:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = len(unified_node)\n",
    "            unified_node[key]=len(unified_node)\n",
    "        elif 'hadm' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['hadm']\n",
    "        elif 'prescript' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['prescript']\n",
    "        elif 'icustay' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['icustay']\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "elif ROOT_DIR == 'dxprx':\n",
    "    unified_node= {'PAD':0,'MASK':1,'CLS':2,'hadm':3,'diagnoses_icd9_code':4,'diagnoses':5,'procedures_icd9_code':6,'procedures':7}\n",
    "    for key in nodes:\n",
    "        if key in literals:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = len(unified_node)\n",
    "            unified_node[key]=len(unified_node)     \n",
    "        elif 'hadm' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['hadm']\n",
    "        elif 'diagnoses_icd9_code' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['diagnoses_icd9_code']\n",
    "        elif 'diagnoses' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['diagnoses']\n",
    "        elif 'procedures_icd9_code' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['procedures_icd9_code']\n",
    "        elif 'procedures' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['procedures']\n",
    "        else:\n",
    "            raise ValueError()\n",
    "            \n",
    "torch.save(unified_node,'{}/unified_node'.format(task))\n",
    "torch.save(node2uninode,'{}/node2uninode'.format(task))\n",
    "print('# Unified nodes : {}'.format(len(unified_node)))\n",
    "\n",
    "for split in DB:\n",
    "    ## Debugging Purpose\n",
    "    #if split == 'train':\n",
    "    #    continue\n",
    "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
    "    if not os.path.isdir(os.path.join(task,split)):\n",
    "        os.mkdir(os.path.join(task,split))\n",
    "    inputs = list()\n",
    "    masks = list()\n",
    "    labels = list()\n",
    "    label_masks = list()\n",
    "    rc_indeces = list()\n",
    "    notes = list()\n",
    "    \n",
    "    for head, note in tqdm(DB[split],total=len(DB[split])):\n",
    "        subgraph = subgraphs[head]\n",
    "        # Append input\n",
    "        inputs.append(list(map(lambda x: node2uninode[x],subgraph)))\n",
    "        # Append label\n",
    "        labels.append(list(map(lambda x: node2uninode[x] if x in literal_id2label else -100,subgraph)))\n",
    "        label_masks.append(list(map(lambda x: 1 if x in literal_id2label else 0,subgraph)))\n",
    "        # Append attention mask for graph encoder\n",
    "        mask =  torch.eye(len(subgraph))\n",
    "        for head_idx, head in enumerate(subgraph):\n",
    "            for tail_idx, tail in enumerate(subgraph):\n",
    "                if head_idx>tail_idx:\n",
    "                    continue\n",
    "                elif (head==0) or (tail==0):\n",
    "                    continue\n",
    "                else:\n",
    "                    if (head,tail) in node2edge:\n",
    "                        mask[(head_idx, tail_idx)]=1.0\n",
    "                        mask[(tail_idx, head_idx)]=1.0\n",
    "        # Add RC index for sample\n",
    "        num_nodes = sum([1 for x in subgraph if x!=0])\n",
    "        rc_index = list()\n",
    "        not_conn = 0\n",
    "        while len(rc_index) < 0.1*num_nodes:\n",
    "            idx_pair = (random.randint(0,num_nodes-1),random.randint(0,num_nodes-1))\n",
    "            node_pair = (subgraph[idx_pair[0]], subgraph[idx_pair[1]])\n",
    "            inv_node_pair = (subgraph[idx_pair[1]], subgraph[idx_pair[0]])\n",
    "            if (idx_pair[0] == idx_pair[1]):\n",
    "                continue\n",
    "            if node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[node_pair],))\n",
    "            elif inv_node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[inv_node_pair],))\n",
    "            else:\n",
    "                if not_conn < 0.05*num_nodes:\n",
    "                    rc_index.append(idx_pair+(len(set(node2edge.values())),))\n",
    "                    not_conn +=1\n",
    "        rc_indeces.append(rc_index)\n",
    "        masks.append(mask)\n",
    "        notes.append(note)\n",
    "    db = {'input':inputs,\n",
    "                'mask':masks,\n",
    "                'label':labels,\n",
    "                'label_mask':label_masks,\n",
    "                'rc_index':rc_indeces,\n",
    "                'text':notes}\n",
    "    torch.save(db,'{}/db'.format(os.path.join(task,split)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "['CLS', 'hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', '\"other b-complex deficiencies\"', '\"anxiety state, unspecified\"', '\"pneumonia, organism unspecified\"', '\"other abnormal glucose\"', '\"unspecified essential hypertension\"', '\"hypovolemia\"', '\"anemia, unspecified\"', '\"asthma, unspecified type, unspecified\"', '\"hyposmolality and/or hyponatremia\"', '\"obstructive sleep apnea (adult)(pediatric)\"', '\"obesity, unspecified\"', '\"alcohol abuse, unspecified\"']\n",
      "mask:\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.])\n",
      "label:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3528, 6172, 275, 2671, 4644, 256, 942, 6588, 7112, 4448, 3255, 3539, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "label_mask:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "rc_index:\n",
      "[('diagnoses', '\"hypovolemia\"', 6), ('CLS', 'diagnoses', 6), ('diagnoses', 'diagnoses_icd9_code', 0), ('\"pneumonia, organism unspecified\"', 'diagnoses_icd9_code', 4)]\n",
      "text:\n",
      "{'dx': '  Hypoxia Community acquired pneumonia Obstructive sleep apnea Asthma Hypertension', 'prx': 'None'}\n"
     ]
    }
   ],
   "source": [
    "IDX = 1\n",
    "id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(ROOT_DIR,'entity2id.txt')).read().splitlines()[1:]}\n",
    "uninode2name = {v:k.split('^^')[0] for k,v in unified_node.items()}\n",
    "for k, v in db.items():\n",
    "    print(f'{k}:')\n",
    "    if k=='input':\n",
    "        print([uninode2name[x] for x in v[IDX] if x!=0])\n",
    "    elif k=='mask':\n",
    "        print(v[IDX][1])\n",
    "    elif k=='rc_index':\n",
    "        print([(uninode2name[db['input'][IDX][h]],uninode2name[db['input'][IDX][t]],r) for h,t,r in v[IDX]])\n",
    "    else:\n",
    "        print(v[IDX])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xo4WacHCveOi"
   },
   "source": [
    "## 2-3. Literal Bucket Prediction _(ongoing..)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUWX_Dxg1jCH"
   },
   "outputs": [],
   "source": [
    "task = 'masked_literal_prediction'\n",
    "if not os.path.isdir(task):\n",
    "    os.mkdir(task)\n",
    "# Build Input\n",
    "DB = {'train':[],'valid':[],'test':[]}\n",
    "for sample in note_aid_pair:\n",
    "    split = np.random.choice(list(DB.keys()),p=[0.8,0.1,0.1])\n",
    "    if (len(split)>0.1*len(note_aid_pair)) and (split in ['valid', 'test']):\n",
    "        split = 'train'\n",
    "    elif (len(split)>0.8*len(note_aid_pair)) and (split in ['train']):\n",
    "        split = np.random.choice(['valid','test'],p=[0.5,0.5])\n",
    "    DB[split].append(sample)\n",
    "\n",
    "# Load Bucket ID\n",
    "literalID2bucketID = torch.load('literalID2bucketID')\n",
    "\n",
    "for split in DB:\n",
    "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
    "    if not os.path.isdir(os.path.join(task,split)):\n",
    "        os.mkdir(os.path.join(task,split))\n",
    "    torch.save(literal_id2label,'{}/id2label'.format(os.path.join(task,split)))\n",
    "    torch.save([note for (head,note) in DB[split]],'{}/note'.format(os.path.join(task,split)))\n",
    "    torch.save({'input':[subgraphs[head] for (head,note) in DB[split]],\n",
    "                'mask':[(~np.isin(np.array(subgraphs[head]),list(literals.values()))).astype(np.int64).tolist() for (head,note) in DB[split]],\n",
    "                'label':[list(map(lambda x: literalID2bucketID[x] if x in literalID2bucketID else -100,subgraphs[head])) for (head,note) in DB[split]]},\n",
    "               '{}/{}/kg_norel'.format(task,split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cl5zew0jvW75"
   },
   "source": [
    "## 2-4. Contrastive Learning _(ongoing..)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNTHhbQZ1jaU"
   },
   "outputs": [],
   "source": [
    "def negative_sampling(input, mask):\n",
    "\n",
    "task = 'masked_literal_prediction'\n",
    "if not os.path.isdir(task):\n",
    "    os.mkdir(task)\n",
    "# Build Input\n",
    "DB = {'train':[],'valid':[],'test':[]}\n",
    "for sample in note_aid_pair:\n",
    "    split = np.random.choice(list(DB.keys()),p=[0.8,0.1,0.1])\n",
    "    if (len(split)>0.1*len(note_aid_pair)) and (split in ['valid', 'test']):\n",
    "        split = 'train'\n",
    "    elif (len(split)>0.8*len(note_aid_pair)) and (split in ['train']):\n",
    "        split = np.random.choice(['valid','test'],p=[0.5,0.5])\n",
    "    DB[split].append(sample)\n",
    "\n",
    "# Re-index literals for labeling\n",
    "literal_id2label = {k:v for (v,k) in enumerate(list(literals.values()))}\n",
    "\n",
    "for split in DB:\n",
    "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
    "    if not os.path.isdir(os.path.join(task,split)):\n",
    "        os.mkdir(os.path.join(task,split))\n",
    "    torch.save(literal_id2label,'{}/id2label'.format(os.path.join(task,split)))\n",
    "    torch.save([note for (head,note) in DB[split]],'{}/note'.format(os.path.join(task,split)))\n",
    "    torch.save({'input':[subgraphs[head] for (head,note) in DB[split]],\n",
    "                'mask':[(~np.isin(np.array(subgraphs[head]),list(literals.values()))).astype(np.int64).tolist() for (head,note) in DB[split]],\n",
    "                'label':[list(map(lambda x: literal_id2label[x] if x in literal_id2label else -100,subgraphs[head])) for (head,note) in DB[split]]},\n",
    "               '{}/{}/kg_norel'.format(task,split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3z4_vqY2rzc"
   },
   "source": [
    "**Supp 1. Save DB in torch.tensor format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwxPUhGr2ruw"
   },
   "outputs": [],
   "source": [
    "# Only for DB in tensor form\n",
    "# Get id sequence of notes\n",
    "\n",
    "print(subgraphs)\n",
    "tensorized_subgraphs = torch.LongTensor([x for x in subgraphs])\n",
    "print(max_len)\n",
    "print(len(subgraphs))\n",
    "print(tensorized_subgraphs[0,:20])\n",
    "print('Saving...')\n",
    "torch.save(tensorized_subgraphs,'subgraph_norel')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supp 2. Check input for debugging purpose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1604245224905,
     "user": {
      "displayName": "Park Sungjin",
      "photoUrl": "",
      "userId": "10892187777297360592"
     },
     "user_tz": -540
    },
    "id": "z6vOs_8HZCiy",
    "outputId": "3da879b2-d0e1-4b72-bfd1-8e6e3b63f6c6"
   },
   "outputs": [],
   "source": [
    "split = 'test'\n",
    "input = subgraphs[DB[split][0][0]]\n",
    "mask = (~np.isin(np.array(subgraphs[DB[split][0][0]]),list(literals.values()))).astype(np.int64).tolist() \n",
    "label = list(map(lambda x: literal_id2label[x] if x in literal_id2label else -100,subgraphs[DB[split][0][0]])) \n",
    "print(input)\n",
    "print(mask)\n",
    "print(label)\n",
    "#print(literals)\n",
    "print(len(literals))\n",
    "print(list(literals.items())[-1])\n",
    "print(list(literal_id2label.items())[-1])\n",
    "#list(literals.values())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7awNjSEwl9E"
   },
   "source": [
    "**Supp 3. Run Depth First Search on KG (Node & Relation)**\n",
    "\n",
    "Root is an admission node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qME9ZZ1UwuSv"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "NUM_SPECIAL_TOKENS = 2\n",
    "\n",
    "def get_childs_withrel(subgraph, depth, heads,node2edge):\n",
    "    temp_seq = list()\n",
    "    temp_heads = list()\n",
    "    for head in heads:\n",
    "        node_set = [(head,tail) for tail in subgraph[depth][head]]\n",
    "        for node_pair in node_set:\n",
    "            temp_seq += ['r'+node2edge[node_pair],node_pair[1]]\n",
    "        temp_heads += subgraph[depth][head]\n",
    "    return temp_seq, temp_heads\n",
    "\n",
    "triples = [x.split() for x in open('train2id.txt').read().splitlines()[1:]]\n",
    "node2edge = {(h,t):r for h,t,r in triples}\n",
    "nodes = {' '.join(x.split()[:-1]):x.split()[-1] for x in open('entity2id.txt').read().splitlines()[1:]}\n",
    "literals = {k:int(v)+NUM_SPECIAL_TOKENS for (k,v) in list(nodes.items()) if '^^' in node[0]}\n",
    "edges = {x.split()[0]:x.split()[1] for x in open('relation2id.txt').read().splitlines()[1:]}\n",
    "\n",
    "# Extract Admission Nodes & Literals\n",
    "adm_node = list()\n",
    "for node in list(nodes.items()):\n",
    "    if 'hadm' in node[0]:\n",
    "        adm_node.append(node[1])   \n",
    "        \n",
    "# Initialize subgraph\n",
    "subgraph_norel = [{node:list() for node in adm_node}]\n",
    "\n",
    "#subgraph_rel = dict(adm_node)\n",
    "#node_dict = list(subgraph_norel.keys())\n",
    "\n",
    "# Depth First Search\n",
    "print('start preprocessing')\n",
    "level = 0\n",
    "while len(triples)>0:\n",
    "    queue = list()\n",
    "    print('level:{}'.format(level))\n",
    "    for triple in tqdm(triples):\n",
    "        if triple[0] in subgraph_norel[level]:\n",
    "            subgraph_norel[level][triple[0]].append(triple[1])\n",
    "            flag = False\n",
    "        else:\n",
    "            flag = True\n",
    "        if flag:\n",
    "            queue.append(triple)\n",
    "    print('{}/{}'.format(len(queue),len(triples)))\n",
    "    new_head = list()\n",
    "    for heads in list(subgraph_norel[level].values()):\n",
    "        new_head+=heads\n",
    "    subgraph_norel.append({k:list() for k in new_head})\n",
    "    triples = queue\n",
    "    level += 1\n",
    "\n",
    "# Build subgraph\n",
    "subgraphs = list()\n",
    "max_len = 0\n",
    "for head in tqdm(list(subgraph_norel[0].keys())):\n",
    "    depth=0\n",
    "    seq = [head]\n",
    "    heads = [head]\n",
    "    while depth<level:\n",
    "        seqs, heads = get_childs_withrel(subgraph_norel,depth,heads,node2edge)\n",
    "        seq += seqs\n",
    "        depth+=1\n",
    "    subgraphs.append(list(map(lambda x: int(x)+NUM_SPECIAL_TOKENS if 'r' not in x else -(int(x.split('r')[-1])+1),seq)))\n",
    "    if len(seq)>max_len:\n",
    "        max_len = len(seq)\n",
    "\n",
    "# Align subgraph and note\n",
    "aid = [nodes['</hadm_id/{}>'.format(x)] for x in open('p_hadm_ids.txt').read().splitlines() if (len(x)>0) and ('</hadm_id/{}>'.format(x) in nodes)]\n",
    "note = [x for x in open('p_sections.txt').read().splitlines() if (len(x)>0)]\n",
    "note_aid_pair = [(x,y) for (x,y) in zip(aid,note) if x in subgraphs]\n",
    "print('{}/{}'.format(len(aid),len(adm_node)))\n",
    "print(len(note))\n",
    "print(len(note_aid_pair))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1-(2). Masked Literal Prediction, Graph Enc, MultiKGenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = '{}_MultiKGenc'.format(ROOT_DIR)\n",
    "UniDirectional = False\n",
    "triples = [x.split() for x in open(os.path.join(ROOT_DIR,'train2id.txt')).read().splitlines()[1:]]\n",
    "node2edge = {(int(h)+NUM_SPECIAL_TOKENS,int(t)+NUM_SPECIAL_TOKENS):r for h,t,r in triples}\n",
    "\n",
    "if not os.path.isdir(task):\n",
    "    os.mkdir(task)\n",
    "\n",
    "# Re-index literals for labeling\n",
    "literal_id2label = {k:v for (v,k) in enumerate(list(literals.values()))}\n",
    "torch.save(literal_id2label,'{}/id2label'.format(task))\n",
    "\n",
    "for split in DB:\n",
    "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
    "    if not os.path.isdir(os.path.join(task,split)):\n",
    "        os.mkdir(os.path.join(task,split))\n",
    "    inputs = list()\n",
    "    masks = list()\n",
    "    labels = list()\n",
    "    label_masks = list()\n",
    "    notes = list()\n",
    "    \n",
    "    for head, note in tqdm(DB[split],total=len(DB[split])):\n",
    "        subgraph = subgraphs[head]\n",
    "        # Append input\n",
    "        inputs.append(subgraph)\n",
    "        # Append label\n",
    "        labels.append(list(map(lambda x: literal_id2label[x] if x in literal_id2label else -100,subgraph)))\n",
    "        label_masks.append(list(map(lambda x: 1 if x in literal_id2label else 0,subgraph)))\n",
    "        # Append attention mask for graph encoder\n",
    "        mask =  torch.stack([torch.eye(len(subgraph)) for _ in range(len(edges))],dim=2)\n",
    "        if UniDirectional:\n",
    "            for head_idx, head in enumerate(subgraph):\n",
    "                for tail_idx, tail in enumerate(subgraph):\n",
    "                    if head_idx>tail_idx:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if (head,tail) in node2edge:\n",
    "                            mask[(head_idx, tail_idx, node2edge[(head,tail)])]=1.0\n",
    "        else:\n",
    "            for head_idx, head in enumerate(subgraph):\n",
    "                for tail_idx, tail in enumerate(subgraph):\n",
    "                    if head_idx>tail_idx:\n",
    "                        continue\n",
    "                    elif (head==0) or (tail==0):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if (head,tail) in node2edge:\n",
    "                            mask[(head_idx, tail_idx, int(node2edge[(head,tail)]))]=1.0\n",
    "                            mask[(tail_idx, head_idx, int(node2edge[(head,tail)]))]=1.0\n",
    "        masks.append(mask)\n",
    "        notes.append(note)\n",
    "            \n",
    "    torch.save({'input':inputs,\n",
    "                'mask':masks,\n",
    "                'label':labels,\n",
    "                'label_mask':label_masks,\n",
    "                'text':notes},\n",
    "                '{}/db'.format(os.path.join(task,split)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "triple2subgraph.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}